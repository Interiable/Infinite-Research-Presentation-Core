나는 선행 UX를 연구하는 팀에서 Physical AI UX 과제에 속해있고 그 속에서 expressive motion design 및 개발 담당이야. 이와 관련된 내용은 linked folder 중 My-AI-Agent/data의 문서 중 몇몇 문서에 구체적으로 표시되어 있어. 

 해당 과제 초기에 새모양 table top 로봇을 AI랑 연결시켜서 간단히 테스트해본 프로젝트가 linked folder 중  robot/birds_robot_project_v1 에 있는 프로젝트야. 그리고 이 프로젝트를 통해 로봇을 테스트해보고 올해 과제 중 나의 역할에 해당하는 부분은 어떻게 진행하면 될지 초기 계획을 세운게 data/reference 폴더에 있는 PoC_Motion_Design_Dev.pdf 파일이야. 

 나는 이 다음단계로 구체적으로 어떻게 expressive motion관련 연구와 이론들을 실제 로봇 제어에 적용할지 많은 연구를 진행했어. 그 연구 과정에서 생성된 문서둘이 data 폴더 여기저기에 많이 있어. 기존 python 기반 로봇제어 프로그램을 ROS2 환경으로 이전하고 BT(Behavior Tree), DMP(dynamic movement primitives), Utility 등 자연스러운 실시간 robot제어를 위해 필요한 개발적인 요소들에 Think of Mind, Laban Movement Analysis, Physical AI UX 10 원칙, 기능과 표현의 비율을 어떻게 적용할지 구체적으로 정의해야해. 이것은 HRI, Expressive Robot Motion 분야에서 사용하는 다양한 이론들을 모두 적용하고 또 그것들이 효과적으로 살아있는듯이 실시간으로 동작할 수 있도록 ROS2기반 로봇 프레임워크로 완성되고 적용되어 실제 로봇을 동작시키고 expressive 요소들을 직접 조절하면서 정답에 가까운 Expressive motion을 디자인 하기 위한 프레임워크를 만드는 과정이야. 이 로봇은 얼굴에 카메라가 달려서 앞에 있는 사람을 볼 수 있고, 시선을 공유할수동 있고 손동작을 알수도 있지. IMU센서로 누가 건드리면 알 수 있고, ToF로 뭔가 가까이오는것도 알아, Multi Mic array로 어디서 소리가 났는지도 알고, 사람이 말하면 그 말도 이해하지. 아, LLM이 연결되어 있어서 이러한 센서와 다양한 data 기반하여 환경, 주변 context를 이해하고 어떤 mode가 되어야할지, 어느정도로 적극적이어야 하는지 등을 판단하기도 하고, 직접 실시간으로 모션을 생성하기도 할거야. 물론 평소는 빠르고 즉각적인 반응을 해야하기 때문에 ROS2에 잘 정의된 모션 로직으로 상황에 맞게 빠르게 동작할거야. 아무튼, 이러한 방향으로 Framework를 설계 한다고 할 때, 내가 다양한 Expressive Motion 연구를 테스트 해보고 위해 필요한 주요 요소들을 잘 정리해주고, 그 요소들이 잘 적용된 robot framework가 되기 위해 어떤 ROS2기반 architecture가 되어야 하고, 그 architecture에 각 요소들은 어떻게 적용되는지, 그 적용된 결과 예시로 어떤 테스트를 수행하면서 expressive motion design을 효과적으로 할 수 있는지를 깊이있게 연구해서 효과적인 인포그래픽 발표자료를 만들어줘. 매우 깊이 있는 연구가 될거고 따라서 완료된 문서도 분량이 많을거고, slide도 꽤 많을 수 있어. slide가 많아도 상관없어. 일단 충분히 내용을 효과적으로 논리적으로 잘 이어지게 납득이 되고 모두가 공감할 수 있도록 잘 작성해줘. 해당 slide는 UX 디자이너들에게 공유될거야. UX 디자이너들은 개발자는 아니지만 기술관련 이해도가 높은 인간공학, 산업공학 쪽 석박사들이야. 개발자만큼 기술적으로 어려우면 안되겠지만 그래도 기술적인 내용이 필요하면 충분히 작성해도 좋아. 그럼 잘 부탁할게. 
